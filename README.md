# Clinical Trials ELT Pipeline

This project implements an ELT (Extract, Load, Transform) pipeline for clinical trial data from various sources.

## Technology Stack

*   **Workflow Orchestration:** Apache Airflow
*   **Data Extraction & Loading:** Python
*   **Environment Variable Management:** `python-dotenv` (for local development)
*   **Data Warehouse:** Snowflake
*   **Data Transformation:** dbt (Data Build Tool)
*   **Cloud Storage (Optional):** AWS S3 / Cloudflare R2 (via `boto3`)

## Data Sources

*   **ISRCTN:** [API Docs](https://bit.ly/ISRCTN_API_docs_v0_6_docx)
*   **EUCTR:** [Scraper](https://github.com/Bilalkamal/EU-CTR-Scraper-v2)
*   **ICTRP:** [Retrieval Tool](https://github.com/gertvv/ictrp-retrieval)
*   **EMA:** [Scraper](https://github.com/MiqG/EMA-Data-Scratching-with-RSelenium)

## Project Structure

An `.env` file (gitignored) is used for managing environment variables locally, loaded by `python-dotenv`. An `.env.example` template is provided.

```
ClinicalTrialsELT/
├── .git/                           # (Implicit version control directory)
├── .gitignore                      # (Recommended: Exclude venv, secrets, logs, data/*, external_tools/*, etc.)
├── .env.example                    # Template for environment variables
├── airflow/
│   ├── dags/
│   │   ├── extraction_dag.py
│   │   └── dbt_dag.py
│   ├── plugins/                    # For custom Airflow plugins
│   │   └── .gitkeep                # Ensures directory is tracked if empty
│   │   └── .gitkeep                # (Actual data files should be gitignored)
│   ├── logs/                       # (Generated by Airflow, should be gitignored)
│   └── airflow.cfg                 # (Generated by Airflow, gitignore or use a template)
├── data/
│   └── raw/                        # For storing extracted raw data files locally (before/if not uploaded)
│       └── .gitkeep                # (Actual data files should be gitignored)
├── dbt_project/
│   ├── analyses/                   # For dbt SQL analyses
│   │   └── .gitkeep
│   ├── macros/                     # For dbt macros
│   │   └── .gitkeep
│   ├── models/
│   │   ├── staging/                # Source-specific staging models
│   │   │   ├── .gitkeep            # Ensures directory is tracked
│   │   │   └── sources.yml         # Example: define raw data sources here
│   │   └── marts/                  # Conformed data models (e.g., dim_trials, fact_outcomes)
│   │       └── .gitkeep            # Ensures directory is tracked
│   ├── seeds/                      # For dbt seed files (CSV data)
│   │   └── .gitkeep
│   ├── snapshots/                  # For dbt snapshots (e.g., SCD type 2)
│   │   └── .gitkeep
│   ├── tests/                      # For custom dbt data tests
│   │   └── .gitkeep
│   ├── dbt_project.yml             # Main dbt project configuration
│   └── profiles.yml.example        # Example dbt profiles configuration for developers
├── external_tools/                 # For housing external scraper/retrieval tool repositories
│   └── .gitkeep                    # (Actual tool contents should be gitignored or managed as submodules)
├── scripts/
│   ├── __init__.py                 # Makes 'scripts' a Python package
│   ├── extraction/
│   │   ├── __init__.py             # Makes 'extraction' a sub-package
│   │   ├── isrctn_extractor.py
│   │   ├── euctr_extractor.py
│   │   ├── ictrp_extractor.py
│   │   └── ema_extractor.py
│   ├── loading/
│   │   ├── __init__.py             # Makes 'loading' a sub-package
│   │   └── load_to_snowflake.py
│   └── utils/
│       ├── __init__.py             # Makes 'utils' a sub-package
│       └── s3_uploader.py          # Module for S3/R2 uploads
├── venv/                           # (Python virtual environment, should be gitignored)
├── requirements.txt                # Python package dependencies
└── README.md                       # This file
```

## Setup and Configuration

1.  **Clone the repository.**
2.  **Set up Python virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```
3.  **Configure Environment Variables:**
    *   Copy `.env.example` to a new file named `.env` in the project root.
    *   Edit `.env` and fill in your actual credentials and configuration values for Snowflake, R2/S3, etc.
    *   The scripts (`s3_uploader.py`, `load_to_snowflake.py`, `isrctn_extractor.py`) will load these variables using `python-dotenv`.
4.  **Configure External Tools:**
    *   Place or clone the EUCTR, ICTRP, and EMA scraper/retrieval tools into the `external_tools/` directory.
    *   Update paths in the respective extractor scripts in `scripts/extraction/` if necessary.
    *   Ensure any dependencies for these external tools are met (e.g., R and RSelenium for the EMA R script).
5.  **Configure Snowflake Connection (if not using .env for scripts):**
    *   **For Python scripts (`load_to_snowflake.py`):** Set environment variables:
        *   `SNOWFLAKE_USER`
        *   `SNOWFLAKE_PASSWORD`
        *   `SNOWFLAKE_ACCOUNT`
        *   Optionally: `SNOWFLAKE_WAREHOUSE`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_SCHEMA` (defaults are provided in the script).
    *   **For Airflow:** Create a Snowflake connection in the Airflow UI (e.g., `snowflake_default` as referenced in `extraction_dag.py`).
    *   **For dbt:** Copy `dbt_project/profiles.yml.example` to `dbt_project/profiles.yml` (or `~/.dbt/profiles.yml`) and fill in your Snowflake credentials. Ensure the `DBT_PROFILES_DIR` in `airflow/dags/dbt_dag.py` points to the correct location of your `profiles.yml` for Airflow execution.
6.  **Configure Cloudflare R2 / S3-compatible Storage (if not using .env for scripts):**
    *   If you want to upload extracted raw XML files to R2/S3, set the following environment variables:
        *   `R2_ENDPOINT_URL`: Your R2 S3 API endpoint (e.g., `https://<your_account_id>.r2.cloudflarestorage.com`)
        *   `R2_ACCESS_KEY_ID`: Your R2 Access Key ID.
        *   `R2_SECRET_ACCESS_KEY`: Your R2 Secret Access Key.
        *   `R2_ISRCTN_BUCKET`: The R2 bucket name where ISRCTN data should be stored (used by `isrctn_extractor.py`).
        *   (Similar bucket environment variables might be needed for other extractors if they also use the `s3_uploader.py` module, e.g., `R2_EUCTR_BUCKET`).
    *   The `s3_uploader.py` module uses these variables to connect and upload files.
7.  **Configure Airflow:**
    *   Ensure Airflow is installed and configured.
    *   Place the DAGs from `airflow/dags/` into your Airflow DAGs folder.
    *   Make sure the `PROJECT_ROOT` and `DBT_PROJECT_DIR` paths in the DAGs are correct for your Airflow environment.
    *   Ensure the Airflow worker has access to Python dependencies and any external tools/interpreters (like Rscript).
8.  **Run the Pipeline:**
    *   The `clinical_trials_extraction_load` DAG will extract data and load it to raw Snowflake tables.
    *   The `clinical_trials_dbt_transform` DAG will run dbt models to transform the raw data.

## Development Notes

*   **Environment Variables**: For local script execution outside of Airflow, environment variables are loaded from the `.env` file. For Airflow DAG execution, ensure these variables are available to the Airflow worker environment (e.g., through Airflow UI, a secrets backend, or by ensuring the `.env` file is accessible and loaded by the worker, though the latter is less common for production).
*   **Raw Data Storage:** Extracted files are saved locally to `data/raw/<source_name>/`. If R2/S3 is configured, these files are also uploaded to the specified bucket and prefix.
*   **Local File Deletion (Post-Upload):** The `isrctn_extractor.py` currently keeps local copies after uploading to R2. You can uncomment the `os.remove(local_xml_path)` line in `_save_xml_response_and_upload` if you wish to delete them after a successful upload.
*   **Extractor Scripts:** 
    *   Modify the extractor functions in `scripts/extraction/` to save their output to `data/raw/<source_name>/<filename>.xml` (or JSON) and return the full path to the saved file. 
    *   The `isrctn_extractor.py` now integrates the R2 upload. Other extractors can be similarly updated to use `scripts.utils.s3_uploader.upload_file_to_r2`.
*   **dbt Models:**
    *   Define sources in `dbt_project/models/staging/sources.yml` pointing to your raw Snowflake tables.
    *   Develop staging models in `dbt_project/models/staging/` to clean and parse the raw VARIANT data.
    *   Develop mart models in `dbt_project/models/marts/` for the final conformed data structures. 